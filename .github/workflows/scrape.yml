name: Scrape Limit-up Stocks

on:
  schedule:
    - cron: '10 15 * * *'  # 每天 UTC 7:30 (北京时间 15:30) 运行
  workflow_dispatch:  # 允许手动触发

jobs:
  scrape:
    runs-on: ubuntu-latest
    permissions:
      contents: write  # 授予写入仓库内容的权限
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v2
      with:
        fetch-depth: 0  # 获取完整历史以便推送
    
    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: '3.9'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests pytz urllib3 chardet
    
    - name: Run scraper
      env:
        TZ: 'Asia/Shanghai'  # 设置时区为北京时间
      run: |
        python scraper.py
    
    - name: List generated files
      run: |
        echo "Generated files:"
        ls -la data/
        echo "HTML files:"
        ls -la *.html
      continue-on-error: true  # 即使此步骤失败也继续执行
    
    - name: Configure Git
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
    
    - name: Commit and push if changed
      run: |
        git add -A
        git diff --quiet && git diff --staged --quiet || (git commit -m "Update limit-up stocks data: $(date +'%Y-%m-%d')" && git push)
    
    - name: Check GitHub Pages status
      run: |
        echo "GitHub Pages will automatically update with the new content."
        echo "You can check the status in the repository's 'Settings' > 'Pages' section."
      continue-on-error: true

  # 可选：添加通知任务
  notify:
    needs: scrape
    runs-on: ubuntu-latest
    if: always()  # 无论上面的任务成功还是失败都运行
    steps:
    - name: Notify completion status
      run: |
        if [ "${{ needs.scrape.result }}" == "success" ]; then
          echo "Scraping and updating completed successfully!"
        else
          echo "There was an issue with the scraping process."
        fi
      continue-on-error: true

